defaults:
  - model_settings: codet5_base_2048.yaml # Name of model settings file
  - training_data_settings: mathcomp_random_benchmark_training.yaml # Name of training data settings file
  - training_settings: basic_eval_100 # Name of training settings file
  - override hydra/job_logging: 'disabled'

name: ProofWala-codet5-base-multilingual-random-2048 # Can be any name will be used in logs
experiment_type: 'Training' # Can be 'Training' or 'Inferencing'

model_settings:
  name_or_path: <root>/models/ProofWala-codet5-base-multilingual-random-2048/wt-ct/checkpoint-340000
  logging_dir: <root>/data/proofsteps/training_results/logs/model/multi

training_data_settings:
  training_data_dir:
  training_meta_filename:
  training_data_log_dir: <root>/data/proofsteps/training_results/logs/random
  evals:
    - eval_name: mathcomp
      eval_data_dir: <root>/data/proofsteps/random-big-test/math-comp/eval
      eval_meta_filename: local.meta.json
      eval_data_log_dir: <root>/data/proofsteps/training_results/random/mathcomp/logs/eval/multi
  eval_data_dir:
  eval_meta_filename:
  eval_data_log_dir:
  test_data_dir: <root>/data/proofsteps/random-big-test/math-comp/test
  test_meta_filename: local.meta.json
  test_data_log_dir: <root>/data/proofsteps/training_results/random/mathcomp/logs/test/multi

training_settings:
  training_args:
    resume_from_checkpoint: <root>/models/ProofWala-codet5-base-multilingual-random-2048/wt-ct/checkpoint-340000 # Huggingface bug https://github.com/huggingface/transformers/pull/26739

# Run experiments using the following command:
# nohup torchrun --nproc-per-node 2 src/thrall_lib/main/run.py &
# nohup torchrun --nproc-per-node 2 --master-port 31052  src/thrall_lib/main/run.py &
# The default master port is 29500, but you can change it to any other port number when running multiple experiments on the same machine
# nohup torchrun --nproc-per-node 2 src/thrall_lib/main/run.py model_settings=codet5_small_128 training_data_settings=simple_benchmark_training training_settings=basic_split_09_01 &
